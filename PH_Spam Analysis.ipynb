{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ee173c-441f-4447-ad69-b5a76492e7d6",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) usage to identify scam messages from Kaggle's Philippine Scam SMS data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d6a444-0d56-4e7f-891a-94c535a52cff",
   "metadata": {},
   "source": [
    "## Initial setup of Python libraries and the NLP function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45f8e677-7f49-4cc5-936b-c781b9b6d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from collections import Counter\n",
    "from sklearn.metrics import classification_report,confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline\n",
    "\n",
    "# Function for preprocessing\n",
    "def transform_message(message):\n",
    "    # Remove punctuation\n",
    "    message_not_punc = ''.join([char for char in message if char not in string.punctuation])\n",
    "    # Tokenize and remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    message_clean = [word.lower() for word in message_not_punc.split() if word.lower() not in stop_words]\n",
    "    return message_clean\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove special characters and punctuation, but preserve whole words\n",
    "    cleaned = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation (except spaces and words)\n",
    "    cleaned = cleaned.lower()  # Convert to lowercase\n",
    "    return cleaned\n",
    "    \n",
    "# Full pipeline function for the SVC Algorithm\n",
    "def spam_detection_pipeline(data, text_column, label_column):\n",
    "    # Drop rows with missing values in text_column or label_column\n",
    "    data = data.dropna(subset=[text_column, label_column])\n",
    "\n",
    "    # Vectorization: CountVectorizer -> TfidfTransformer\n",
    "    vectorizer = CountVectorizer(analyzer=transform_message)\n",
    "    X_counts = vectorizer.fit_transform(data[text_column])\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_tfidf, \n",
    "        data[label_column], \n",
    "        test_size=0.3, \n",
    "        random_state=50\n",
    "    )\n",
    "    \n",
    "    # Train the SVM model\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    # Evaluation\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8410b3c3-9037-451b-86bd-c3b39c2829b2",
   "metadata": {},
   "source": [
    "## Using the Support Vector Classification (SVC) algorithm-powered NLP to analyze the Philippine Scam SMS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "513c0d8d-9930-4578-897e-5d9c6474a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text Prediction_Model1  \\\n",
      "0  A new and \"Green\" township will be rising in t...               ham   \n",
      "1  New OFFICE space for sale in Ortigas Avenue ne...              spam   \n",
      "2  Celebrate the season of love with a sweet stay...               ham   \n",
      "3  Shangri-La Updates:\\n\\nRISEmakati\\n\\n-One-of-a...               ham   \n",
      "4  Co in Makati needs Software Java Developer, kn...               ham   \n",
      "\n",
      "  Prediction_Model2  \n",
      "0               ham  \n",
      "1              spam  \n",
      "2               ham  \n",
      "3               ham  \n",
      "4               ham  \n"
     ]
    }
   ],
   "source": [
    "ph_data = pd.read_csv('Datasets/PH_SPAM.csv')\n",
    "messages_ph = ph_data[['text']].copy()\n",
    "\n",
    "# Replace '\\n' with spaces first\n",
    "messages_ph['text'] = messages_ph['text'].str.replace('\\n', ' ', regex=True)\n",
    "\n",
    "# Apply the clean_text function\n",
    "messages_ph['text'] = messages_ph['text'].dropna().apply(clean_text)\n",
    "\n",
    "# Load models from Dataset 1\n",
    "with open('Datasets/UCI_SPAM_Model/vectorizer_dataset1.pkl', 'rb') as vec_file1:\n",
    "    vectorizer1 = pickle.load(vec_file1)\n",
    "\n",
    "with open('Datasets/UCI_SPAM_Model/tfidf_transformer_dataset1.pkl', 'rb') as tfidf_file1:\n",
    "    tfidf_transformer1 = pickle.load(tfidf_file1)\n",
    "\n",
    "with open('Datasets/UCI_SPAM_Model/svm_model_dataset1.pkl', 'rb') as model_file1:\n",
    "    clf1 = pickle.load(model_file1)\n",
    "\n",
    "# Load models from Dataset 2\n",
    "with open('Datasets/Kaggle_SPAM_Model/vectorizer_dataset2.pkl', 'rb') as vec_file2:\n",
    "    vectorizer2 = pickle.load(vec_file2)\n",
    "\n",
    "with open('Datasets/Kaggle_SPAM_Model/tfidf_transformer_dataset2.pkl', 'rb') as tfidf_file2:\n",
    "    tfidf_transformer2 = pickle.load(tfidf_file2)\n",
    "\n",
    "with open('Datasets/KAGGLE_SPAM_Model/svm_model_dataset2.pkl', 'rb') as model_file2:\n",
    "    clf2 = pickle.load(model_file2)\n",
    "\n",
    "# Preprocess and predict with Dataset 1 models\n",
    "X_counts1 = vectorizer1.transform(messages_ph['text'].fillna(''))\n",
    "X_tfidf1 = tfidf_transformer1.transform(X_counts1)\n",
    "predictions1 = clf1.predict(X_tfidf1)\n",
    "\n",
    "# Preprocess and predict with Dataset 2 models\n",
    "X_counts2 = vectorizer2.transform(messages_ph['text'].fillna(''))\n",
    "X_tfidf2 = tfidf_transformer2.transform(X_counts2)\n",
    "predictions2 = clf2.predict(X_tfidf2)\n",
    "\n",
    "# Combine predictions into the dataset\n",
    "results = ph_data.copy()\n",
    "results['Prediction_Model1'] = predictions1\n",
    "results['Prediction_Model2'] = predictions2\n",
    "\n",
    "# Display the first few rows with predictions\n",
    "print(results[['text', 'Prediction_Model1', 'Prediction_Model2']].head())\n",
    "\n",
    "# Print result to another CSV File\n",
    "results.to_csv('Datasets/PH_SPAM_predictions_combined.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1edee9ab-9ae1-4bdb-916e-c70345d90d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model 1 (Dataset 1): 0.28\n",
      "Accuracy of Model 2 (Dataset 2): 0.36\n"
     ]
    }
   ],
   "source": [
    "# Define the true labels (all spam)\n",
    "true_labels = ['spam'] * len(messages_ph)\n",
    "\n",
    "# Calculate accuracy for both models\n",
    "accuracy_model1 = accuracy_score(true_labels, predictions1)\n",
    "accuracy_model2 = accuracy_score(true_labels, predictions2)\n",
    "\n",
    "# Print the accuracies\n",
    "print(f\"Accuracy of Model 1 (Dataset 1): {accuracy_model1:.2f}\")\n",
    "print(f\"Accuracy of Model 2 (Dataset 2): {accuracy_model2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29f982bb-eaef-41f5-b4ed-b459906f9b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misclassified by Model 1:\n",
      "                                                text Prediction_Model1\n",
      "0  a new and green township will be rising in the...               ham\n",
      "2  celebrate the season of love with a sweet stay...               ham\n",
      "3  shangrila updates  risemakati  oneofakind resi...               ham\n",
      "4  co in makati needs software java developer kno...               ham\n",
      "5  co in makati needs software java developer bac...               ham\n",
      "\n",
      "Misclassified by Model 2:\n",
      "                                                text Prediction_Model2\n",
      "0  a new and green township will be rising in the...               ham\n",
      "2  celebrate the season of love with a sweet stay...               ham\n",
      "3  shangrila updates  risemakati  oneofakind resi...               ham\n",
      "4  co in makati needs software java developer kno...               ham\n",
      "5  co in makati needs software java developer bac...               ham\n",
      "\n",
      "Disagreements Between Model 1 and Model 2:\n",
      "                                                 text Prediction_Model1  \\\n",
      "18  pondopesoyou are granted p40000 credit on jan ...               ham   \n",
      "19  the albany by megaworld will be the 1st condo ...               ham   \n",
      "21  pesoloanyou are granted 8000 peso credit on ma...               ham   \n",
      "39  welcome  your have p1222 for sot   web 11ylife...               ham   \n",
      "43  welcome  your have p1222 for sot   web greatsl...               ham   \n",
      "\n",
      "   Prediction_Model2  \n",
      "18              spam  \n",
      "19              spam  \n",
      "21              spam  \n",
      "39              spam  \n",
      "43              spam  \n"
     ]
    }
   ],
   "source": [
    "# Combine predictions and true labels into a DataFrame\n",
    "results = messages_ph.copy()\n",
    "results['True_Label'] = true_labels\n",
    "results['Prediction_Model1'] = predictions1\n",
    "results['Prediction_Model2'] = predictions2\n",
    "\n",
    "# Find misclassifications for each model\n",
    "misclassified_model1 = results[results['Prediction_Model1'] != results['True_Label']]\n",
    "misclassified_model2 = results[results['Prediction_Model2'] != results['True_Label']]\n",
    "\n",
    "# Find disagreements between the two models\n",
    "disagreements = results[results['Prediction_Model1'] != results['Prediction_Model2']]\n",
    "\n",
    "# Display results\n",
    "print(\"Misclassified by Model 1:\")\n",
    "print(misclassified_model1[['text', 'Prediction_Model1']].head())\n",
    "\n",
    "print(\"\\nMisclassified by Model 2:\")\n",
    "print(misclassified_model2[['text', 'Prediction_Model2']].head())\n",
    "\n",
    "print(\"\\nDisagreements Between Model 1 and Model 2:\")\n",
    "print(disagreements[['text', 'Prediction_Model1', 'Prediction_Model2']].head())\n",
    "\n",
    "# Save misclassifications by Model 1\n",
    "misclassified_model1.to_csv('Datasets/misclassified_model1.csv', index=False)\n",
    "\n",
    "# Save misclassifications by Model 2\n",
    "misclassified_model2.to_csv('Datasets/misclassified_model2.csv', index=False)\n",
    "\n",
    "# Save disagreements between Model 1 and Model 2\n",
    "disagreements.to_csv('Datasets/model_disagreements.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b20045-9d54-49d3-bb99-420b6b32840b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
